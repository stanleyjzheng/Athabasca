\documentclass[11pt, letterpaper, twoside]{article}
\usepackage[letterpaper, portrait, left=1in, right=1in, top=1in, bottom=1in]{geometry}\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[explicit]{titlesec}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{inputenc}
\usepackage{enumitem}
\usepackage{booktabs, multirow} %for borders and merged ranges
\usepackage{soul}% for underlines
\usepackage[table]{xcolor} % for cell colors
\usepackage{multicol} % For multiple columns
\setlength{\parindent}{0pt}
\newcommand\aug{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!} %Use \aug to make a equal column for an augmented matrix. Ie. 1 & 2 & 3 & 4 & \aug & x \\
\begin{document}
Linear Algebra Equations and theorems
\vspace{0.2cm}

One parameter linear equation means that there is one variable in the solution. Eg. $\big[\begin{smallmatrix}
	1 & 0 & \frac{2}{a} & \frac{2}{a}\\
	0 & 1 & \frac{2}{a} & \frac{2}{a}
\end{smallmatrix}\big]$

\textbf{Leontief} consumption matrices are productive when \((I-C)^{-1}\). The greatest dollar value is the sector that requires the largest amount of inputs from the other sectors.

\textbf{Symmetrical} matrices are their own transposes.

\textbf{Diagonal} matrices consist of only the main diagonal. It is invertible iff all diagonal entries are nonzero.

\textbf{Triangular} matrices consist of 0's below or above the main diagonal. They can be used to solve systems through back or forwards substitution.

\begin{multicols}{2}
\vspace{2mm}
Properties of matrix arithmetic %Pg 39 in textbook
\begin{enumerate}[label=\alph*)]
\item \(A+B=B+A\)
\item \(A+(B+C)=(A+B)+C\)
\item \(A(BC)=(AB)C\)
\item \(A(B+C)=AB+AC\)
\item \((B+C)A=BA+CA\)
\item \(a(B+C)=aB+aC\)
\item \((a+b)C=aC+bC\)
\item \(a(bC)=(ab)C\)
\item \(a(BC)=(aB)C=B(aC)\)
\item \((A+B)^2=A^2+AB+BA+B^2\)
\end{enumerate}
\columnbreak
Properties of inverse matrices % Pg 46 in textbook
\begin{enumerate}[label=\alph*)]
    \item \((AB)^{-1}=B^{-1}A^{-1}\)
    \item \((A^T)^{-1}=(A^{-1})^T\)
\end{enumerate}

Equivalent statements theorem (all true or all false)
\begin{enumerate}[label=-]
\item \(A\) is invertible.
\item \(A\mathbf{x}=0\) has only the trivial solution
\item The reduced row echelon form of \(A\) is \(I_n\)
\item \(A\) can be expressed as a product of elementary matrices. 
\item \(A\mathbf{x}=\mathbf{b}\) is consistent for every \(n\times1\) matrix \(\mathbf{b}\) and has exactly one solution
\end{enumerate}
\end{multicols}

\vspace{2mm}
If A is an \textbf{invertible} \(m\times n\) matrix, then for each \(n\times1\) matrix \textbf{b}, the systems of equations \(A\textbf{x}=\textbf{b}\) has exactly one solution, namely, \(\textbf{x}=A^{-1}\textbf{b}\)

\vspace{2mm}
A matrix is called \textbf{linearly independent} if the vector equation \(x_1\mathbf{v}_1+\cdots+x_n\mathbf{v_n}=\mathbf{0}\) has \textbf{only} the trivial solution. If it has more solutions, it is not linearly independent.

\vspace{2mm}
For a \textbf{homogenous} system, the system either has only the trivial solution, or more than one solution.

\vspace{2mm}
For a non-homogenous system, either the system has a single unique solution, more than one solution, or no solution at all.

\vspace{2mm}
A homogenous system of \(m\) linear equations in \(n\) unknowns always has a non-trivial solution if \(m<n\).

\vspace{2mm}
A system is \textbf{consistent} when it has at least one solution. If the system \(x_1v_1+\dots + x_nv_n=\mathbf{b}\), we say that \textbf{b} is a linear combination of the vectors. No solutions means the system is inconsistent.

\vspace{2mm}
A \textbf{consistent} system will have a \textbf{unique} solution if and only if the columns of the coefficient matrix are linearly independent vectors (if the homogenous linear equation has non-trivial solutions)

\vspace{2mm}
The \textbf{span} of a system is the sum of the linear combination of each of the column vectors (dimensions)

\vspace{2mm}
A matrix is \textbf{orthogonal} if \(u\cdot v=0\).

\vspace{2mm}
\(||u||=\sqrt{u\cdot u}\)

\vspace{2mm}
\textbf{Cauchy-Schwarz}: \(|u+v|\leq ||u|| ||v||\)

\vspace{2mm}
The \textbf{adjoint method} is finding the inverse of a matrix by multiplying by the inverse of the determinant.

\vspace{2mm}
\textbf{Cramer's rule} is replacing the variable you are trying to find with the solution vector and then dividing by the determinant of the coefficient matrix

\vspace{2mm}
\textbf{Euclidean distance} is the abs value distance/magnitude between two points. \(d=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\)

\vspace{2mm}
\textbf{Cauchy Schwarz} inequality states \(|x\cdot y|\leq ||x||\cdot||y||\) (the dot product is less than the norm) or \(||x||||y||\cos\theta\leq||x||||y||\)

\vspace{2mm}
\textbf{Dot product} is the projection of one matrix multiplied by the length of the other matrix - order does not matter. Sum of element wise multiplication.

\vspace{2mm}
\textbf{Cross product} is the vector that makes the determinant zero, or \(A\times B= ||A||||B||\sin\theta\). 
The length of the resultant vector is the parallelogram's area, and is perpendicular to the other vectors.
\[\begin{bmatrix}
    v_1\\
    v_2\\
    v_3
\end{bmatrix}\times\begin{bmatrix}
    w_1\\
    w_2\\
    w_3
\end{bmatrix}=\det \begin{bmatrix}
    \hat{i}&v_1&w_1\\
    \hat{j}&v_2&w_2\\
    \hat{k}&v_3&w_3
\end{bmatrix}=0\]

\vspace{2mm}
Standard matrix for \textbf{shear} is \(\begin{bmatrix}
    1&k\\
    0&1
\end{bmatrix}\begin{bmatrix}
    x\\
    y\\
\end{bmatrix}\)

\vspace{2mm}
Standard matrix for \textbf{rotation} is \(\begin{bmatrix}
    \cos\theta&-\sin\theta\\
    \sin\theta&\cos\theta
\end{bmatrix}\)

\vspace{2mm}
Standard matrix for \textbf{contraction} is \(k\) times the identity matrix.

\vspace{2mm}

\textbf{Vector axioms}
\begin{enumerate}[label=-]
\item Additive: \(x+y=y+x\), \((x+y)+x=x+(y+x)\), \(0+x=x+0=x\), \((-x)+x=x+(-x)=0\)
\item Multiplicative: \(0x=0\), \(1x=x\), \((cd)x=c(dx)\)
\item Distributive: \(c(x+y)=cx+cy\), \((c+d)x=(cx+dx)\)
\end{enumerate}
\vspace{2mm}
Extras:
\[\begin{bmatrix}
a&b\\
c&d
\end{bmatrix}\begin{bmatrix}
e&f\\
g&h
\end{bmatrix}=\begin{bmatrix}
ae+bg&af+bh\\
ce+dg&cf+dh
\end{bmatrix}\]

A system of linear equations with coefficient matrix \(A\) will be inconsistent for certain values on the right hand side if the row echelon form of \(A\) contains a row of zeros. If the row echelon form of the coefficient matrix \(A\) does not contain a row of zeros, then the system is always consistent, regardless of what the right hand side is.

Standard matrix for shear, reflection of certain dimensions

Eigenvectors/eigenvalues:
https://www.scss.tcd.ie/Rozenn.Dahyot/CS1BA1/SolutionEigen.pdf

Eigenbasis makes powers much easier since it is a diagonal matrix. 
You can find eigenbasis by multiplying by the tranformation on the right, and the 
inverse of the transformation on the left. 
Then, you compute the power, and convert back.
This is called "change of basis"

\end{document}