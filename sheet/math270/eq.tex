\documentclass[11pt, letterpaper, twoside]{article}
\usepackage[letterpaper, portrait, left=1in, right=1in, top=1in, bottom=1in]{geometry}\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[explicit]{titlesec}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{inputenc}
\usepackage{enumitem}
\usepackage{booktabs, multirow} %for borders and merged ranges
\usepackage{soul}% for underlines
\usepackage[table]{xcolor} % for cell colors
\usepackage{multicol} % For multiple columns
\setlength{\parindent}{0pt}
\newcommand\aug{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!} %Use \aug to make a equal column for an augmented matrix. Ie. 1 & 2 & 3 & 4 & \aug & x \\
\begin{document}
Linear Algebra Equations and theorems
\vspace{0.2cm}

One parameter linear equation means that there is one variable in the solution. Eg. $\big[\begin{smallmatrix}
	1 & 0 & \frac{2}{a} & \frac{2}{a}\\
	0 & 1 & \frac{2}{a} & \frac{2}{a}
\end{smallmatrix}\big]$

\textbf{Leontief} consumption matrices are productive when \((I-C)^{-1}\). The greatest dollar value is the sector that requires the largest amount of inputs from the other sectors.

\textbf{Symmetrical} matrices are their own transposes.

\textbf{Diagonal} matrices consist of only the main diagonal. It is invertible iff all diagonal entries are nonzero.

\textbf{Triangular} matrices consist of 0's below or above the main diagonal. They can be used to solve systems through back or forwards substitution.

\begin{multicols}{2}
\vspace{2mm}
Properties of matrix arithmetic %Pg 39 in textbook
\begin{enumerate}[label=\alph*)]
\item \(A+B=B+A\)
\item \(A+(B+C)=(A+B)+C\)
\item \(A(BC)=(AB)C\)
\item \(A(B+C)=AB+AC\)
\item \((B+C)A=BA+CA\)
\item \(a(B+C)=aB+aC\)
\item \((a+b)C=aC+bC\)
\item \(a(bC)=(ab)C\)
\item \(a(BC)=(aB)C=B(aC)\)
\item \((A+B)^2=A^2+AB+BA+B^2\)
\end{enumerate}
\columnbreak
Properties of inverse matrices % Pg 46 in textbook
\begin{enumerate}[label=\alph*)]
    \item \((AB)^{-1}=B^{-1}A^{-1}\)
    \item \((A^T)^{-1}=(A^{-1})^T\)
\end{enumerate}

Equivalent statements theorem (all true or all false)
\begin{enumerate}[label=-]
\item \(A\) is invertible.
\item \(A\mathbf{x}=0\) has only the trivial solution
\item The reduced row echelon form of \(A\) is \(I_n\)
\item \(A\) can be expressed as a product of elementary matrices. 
\item \(A\mathbf{x}=\mathbf{b}\) is consistent for every \(n\times1\) matrix \(\mathbf{b}\) and has exactly one solution
\end{enumerate}
\end{multicols}

\vspace{2mm}
If A is an \textbf{invertible} \(m\times n\) matrix, then for each \(n\times1\) matrix \textbf{b}, the systems of equations \(A\textbf{x}=\textbf{b}\) has exactly one solution, namely, \(\textbf{x}=A^{-1}\textbf{b}\)

\vspace{2mm}
A matrix is called \textbf{linearly independent} if the vector equation \(x_1\mathbf{v}_1+\cdots+x_n\mathbf{v_n}=\mathbf{0}\) has \textbf{only} the trivial solution. If it has more solutions, it is not linearly independent.

\vspace{2mm}
For a \textbf{homogenous} system, the system either has only the trivial solution, or more than one solution.

\vspace{2mm}
For a non-homogenous system, either the system has a single unique solution, more than one solution, or no solution at all.

\vspace{2mm}
A homogenous system of \(m\) linear equations in \(n\) unknowns always has a non-trivial solution if \(m<n\).

\vspace{2mm}
A system is \textbf{consistent} when it has at least one solution. If the system \(x_1v_1+\dots + x_nv_n=\mathbf{b}\), we say that \textbf{b} is a linear combination of the vectors. No solutions means the system is inconsistent.

\vspace{2mm}
A \textbf{consistent} system will have a \textbf{unique} solution if and only if the columns of the coefficient matrix are linearly independent vectors (if the homogenous linear equation has non-trivial solutions)

\vspace{2mm}
The \textbf{span} of a system is the sum of the linear combination of each of the column vectors.

\vspace{2mm}
A matrix is \textbf{orthogonal} if \(u\cdot v=0\).

\vspace{2mm}
\(||u||=\sqrt{u\cdot u}\)

\vspace{2mm}
\textbf{Cauchy-Schwarz}: \(|u+v|\leq ||u|| ||v||\)

\vspace{2mm}
The \textbf{adjoint method} is finding the inverse of a matrix by multiplying by the inverse of the determinant.

\vspace{2mm}
\textbf{Cramer's rule} is replacing the variable you are trying to find with the solution vector and then dividing by the determinant of the coefficient matrix

\vspace{2mm}
\textbf{Euclidean distance} is the abs value distance/magnitude between two points. \(d=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\)

\vspace{2mm}
Extras:

\vspace{2mm}
A system of linear equations with coefficient matrix \(A\) will be inconsistent for certain values on the right hand side if the row echelon form of \(A\) contains a row of zeros. If the row echelon form of the coefficient matrix \(A\) does not contain a row of zeros, then the system is always consistent, regardless of what the right hand side is.

Vector operations (what does dot/cross product represent and how to find)

Cauchy-Schwarz inequality for vectors

Vector Space Axioms

Contraction and dilation of matrices

Standard matrix for shear, reflection of certain dimensions

\end{document}